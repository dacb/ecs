{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install sphinx\n",
    "\n",
    "navigate to ECS_demo/\n",
    "\n",
    "touch core.py\n",
    "\n",
    "cut and paste all that code stuff below into core.py\n",
    "\n",
    "cd ../docs/\n",
    "\n",
    "sphinx-autogen index.rst\n",
    "\n",
    "make html\n",
    "\n",
    "you got an error! : pip install sphinx_rtd_theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *pythonic* package development with the trimmings\n",
    "\n",
    "## Overview\n",
    "\n",
    "By the grace of open-source-dev there are several free lunches you should know of:\n",
    "\n",
    "2. [sphinx](http://www.sphinx-doc.org/en/master/)\n",
    "    1. sphinx can be a bit [finicky](https://samnicholls.net/2016/06/15/how-to-sphinx-readthedocs/). The most important feature to introduce to you to today will be \n",
    "    2. [autodocs](https://samnicholls.net/2016/06/15/how-to-sphinx-readthedocs/) where we generate documentation from just your \n",
    "    3. [docstrings](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) super cool!\n",
    "1. [read the docs](https://readthedocs.org/)\n",
    "    1. not only is it free, but read the docs has a magnicent protocol for turning your hard-earned digital documentation to a pdf. Possibly my favorite feature I'll mention today.\n",
    "3. [travis CI](https://travis-ci.org/)\n",
    "    1. \"CI\" stands for continuous integration. These folks provide you with a free service -- up to 1 hour of CPU time on their servers to run all of your unit tests. \n",
    "4. [pypi](https://pypi.org/)\n",
    "    1. You want people using your code as fast as possible, right? \n",
    "5. [coveralls](https://coveralls.io/)\n",
    "    1. how much of that passed build is covered?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sphinx\n",
    "\n",
    "1. clone this repo. \n",
    "\n",
    "2. cd into the main directory and checkout the directory structure with tree:\n",
    "\n",
    "```\n",
    "$ tree\n",
    ".\n",
    "├── ECS_demo\n",
    "│   ├── __init__.py\n",
    "│   ├── core.py\n",
    "│   ├── data\n",
    "│   │   ├── climate_sentiment_m1.h5\n",
    "│   │   └── tweet_global_warming.csv\n",
    "│   ├── input.py\n",
    "│   ├── tests\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── test_ECS_demo.py\n",
    "│   └── version.py\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "├── appveyor.yml\n",
    "├── docs\n",
    "│   ├── Makefile\n",
    "│   ├── _static\n",
    "│   ├── conf.py\n",
    "│   ├── index.rst\n",
    "│   └── source\n",
    "│       ├── ECS_demo.core.rst\n",
    "│       ├── ECS_demo.rst\n",
    "│       └── ECS_demo.tests.rst\n",
    "├── examples\n",
    "│   ├── README.ipynb\n",
    "│   └── README.txt\n",
    "├── requirements.txt\n",
    "└── setup.py\n",
    "```\n",
    "\n",
    "3. We're about to find out just how busy this directory structure can be with these added open source features. But for now, the main project lives under `ECS_demo/` with `tests/` and `data/` subdirectories.\n",
    "\n",
    "4. Go ahead and inspect the contents of the core.py and test_ECS_demo.py files, in case you're interested. There's some common elements here in the package development world. `core.py` contains, well, the core code of the package. In a larger package you might have other modules living here such as `analysis.py` or `visualize.py`, depending on how you want to organize your code. For now, the `core.py` file contains four functions: `load_data, data_setup, baseline_model` and one class: `Benchmark`. You can learn more about pythonic naming conventions from the [pep8](https://www.python.org/dev/peps/pep-0008/) documentation.\n",
    "\n",
    "5. Time to get to Sphinx! cd over to the docs directory. In this tutorial, I've setup the appropriate rst files already. I haven't had excellent luck with using sphinx-quickstart or sphinx-autogen, personally. And so I will always start with a template such as this and modify the `.rst` files as needed. Suffice to say, if you are interested in creating your documentation from scratch I found this [source](https://samnicholls.net/2016/06/15/how-to-sphinx-readthedocs/) helpful.\n",
    "\n",
    "6. All you need to do is type `make html` in the `docs/` directory where your `Makefile` is sitting. and Sphinx will generate static html documents of your site.\n",
    "\n",
    "```\n",
    "$ tree -L 2\n",
    ".\n",
    "├── Makefile\n",
    "├── _build\n",
    "│   ├── doctrees\n",
    "│   └── html\n",
    "├── _static\n",
    "├── conf.py\n",
    "├── index.rst\n",
    "└── source\n",
    "    ├── ECS_demo.core.rst\n",
    "    ├── ECS_demo.rst\n",
    "    └── ECS_demo.tests.rst\n",
    "```\n",
    "\n",
    "7. Use your preferred browser to checkout your site: `open _build/html/index.html` \n",
    "\n",
    "![](demo1.png)\n",
    "\n",
    "8. If you navigate \n",
    "3. next we'll do autodocs. make sure the autodoc command works. we'll put the html files into a docs/ directory\n",
    "4. after we have the docs/ directory populated, it's time to upload to readthedocs. \n",
    "5. travis ci and coveralls next. these babies take time to build on virtual machines, this will usher in a good demo of creating 'slim' unit tests\n",
    "6. lastly we'll cover pypi, the dist repo we'll be most concerned with\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "1. sdist vs bdist\n",
    "2. GCP\n",
    "3. API's (twitter for one)\n",
    "\n",
    "put this in core.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleybeckner/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from os.path import dirname, join\n",
    "import sys\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "\n",
    "def load_data(data_file_name, h5File=False):\n",
    "    \"\"\"\n",
    "    Loads data from module_path/data/data_file_name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_name : string\n",
    "        name of csv file to be loaded from module_path/data/\n",
    "        data_file_name.\n",
    "    h5File : boolean, optional, default = False\n",
    "        if True opens hdf5 file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Pandas DataFrame\n",
    "    \"\"\"\n",
    "    module_path = dirname(__file__)\n",
    "    if h5File:\n",
    "        data = load_model(join(module_path, 'data', data_file_name))\n",
    "    else:\n",
    "        with open(join(module_path, 'data', data_file_name), 'rb') as csv_file:\n",
    "            data = pd.read_csv(csv_file, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_setup(top_words=1000, max_words=150):\n",
    "    \"\"\"\n",
    "    preprocesses the twitter climate data. Does things like changes output\n",
    "    to one hot encoding, performs word embedding/padding\n",
    "    :return:\n",
    "    X and Y arrays of data\n",
    "    \"\"\"\n",
    "    data = load_data(\"tweet_global_warming.csv\")\n",
    "    print(\"Full dataset: {}\".format(data.shape[0]))\n",
    "    data['existence'].fillna(value='ambiguous',\n",
    "                             inplace=True)  # replace NA's in existence with \"ambiguous\"\n",
    "    data['existence'].replace(('Y', 'N'), ('Yes', 'No'),\n",
    "                              inplace=True)  # rename so encoder doesnt get confused\n",
    "    data = data.dropna()  # now drop NA values\n",
    "    print(\"dataset without NaN: {}\".format(data.shape[0]))\n",
    "    X = data.iloc[:, 0]\n",
    "    Y = data.iloc[:, 1]\n",
    "    print(\"Number of unique words: {}\".format(len(np.unique(np.hstack(X)))))\n",
    "\n",
    "    # one hot encoding = dummy vars from categorical var\n",
    "    # Create a one-hot encoded binary matrix\n",
    "    # N, Y, Ambig\n",
    "    # 1, 0, 0\n",
    "    # 0, 1, 0\n",
    "    # 0, 0, 1\n",
    "\n",
    "    # encode class as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "\n",
    "    # convert integers to one hot encoded\n",
    "    Y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "    # convert X to ints (y is already done)\n",
    "    token = Tokenizer(num_words=top_words,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True,\n",
    "                      split=' ', char_level=False, oov_token=None)\n",
    "    token.fit_on_texts(texts=X)\n",
    "    X = token.texts_to_sequences(texts=X)\n",
    "    X = sequence.pad_sequences(X, maxlen=max_words)\n",
    "    return X, Y\n",
    "\n",
    "def baseline_model(top_words=1000, max_words=150, filters=32):\n",
    "    \"\"\"\n",
    "    baseline model developed by sarah. so ask her!\n",
    "    :return:\n",
    "    model object\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words + 1, filters,\n",
    "                        input_length=max_words))  # is it better to preconvert using word to vec?\n",
    "    model.add(Convolution1D(filters=filters, kernel_size=3, padding='same',\n",
    "                            activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class Benchmark:\n",
    "    \"\"\"\n",
    "    benchmark method used by the unittests\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def run(function):\n",
    "        timings = []\n",
    "        stdout = sys.stdout\n",
    "        for i in range(5):\n",
    "            sys.stdout = None\n",
    "            startTime = time.time()\n",
    "            function()\n",
    "            seconds = time.time() - startTime\n",
    "            sys.stdout = stdout\n",
    "            timings.append(seconds)\n",
    "            mean = statistics.mean(timings)\n",
    "            print(\"{} {:3.2f} {:3.2f}\".format(\n",
    "                1 + i, mean,\n",
    "                statistics.stdev(timings, mean) if i > 1 else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this in core_test.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import core\n",
    "import unittest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class testKerasModels(unittest.TestCase):\n",
    "\n",
    "    def test_baseline_model(self):\n",
    "        X, Y = core.data_setup()\n",
    "        model = core.baseline_model()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2,\n",
    "                  batch_size=128, verbose=0)\n",
    "\n",
    "        # Final evaluation of the model\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "    def test_benchmark(self):\n",
    "        core.Benchmark.run(self.test_baseline_model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
